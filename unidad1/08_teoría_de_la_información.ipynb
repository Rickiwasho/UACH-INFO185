{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from scipy import fftpack\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from style import *\n",
    "\n",
    "def color2bw(img):\n",
    "    return np.dot(img, [0.299, 0.587, 0.114]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoría de la Información\n",
    "\n",
    "## Contenidos\n",
    "\n",
    "- Cantidad de información y entropía\n",
    "- Codificación de fuente\n",
    "- Codificación de canal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Codificación de fuente\n",
    "\n",
    "- Es el proceso que asigna un código a cada mensaje\n",
    "- El largo de las \"palabras de código\" puede ser **fijo** o **variable**\n",
    "- **Ejemplo:** Queremos codificar las letras del alfabeto (27) usando dos símbolos (código binario)\n",
    "    - Para largo fijo necesitamos al menos 5 bits ($2^5 = 32$)\n",
    "- Pero algunas letras se ocupan más que otras (*e.g.* vocales) \n",
    "- ¿Podemos aprovechar esto para comprimir un mensaje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "with open('quijote.txt', 'r') as file:\n",
    "    texto = file.read()\n",
    "print(texto)\n",
    "texto = texto.translate({ord(k): None for k in string.digits})\n",
    "for symbol in string.punctuation:\n",
    "    texto = texto.replace(symbol, \"\")\n",
    "texto = texto.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "texto = texto.lower().encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(\"\\nLargo del texto: {0}, Cantidad de bits: {1}\".format(len(texto), len(texto)*5))\n",
    "Counter(texto).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Intuición:** Podríamos reducir la cantidad de bits si usamos códigos más cortos para las letras más frecuentes\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teoría de la información\n",
    "\n",
    "- Estudio matemático sobre la cuantificación y transmisión de la información \n",
    "- Propuesto por **Claude Shannon** en 1948: *A Mathematical Theory of Communication*\n",
    "- Proporciona medidas para describir la información de un proceso: **Entropía** e **Información Mutua**\n",
    "- Aplicaciones en telecomunicaciones, computación y biología (genética)\n",
    "- Fuerte influencia en la teoría de codificación y compresión\n",
    "\n",
    "### Las dos fuentes\n",
    "\n",
    "Sean dos fuentes **F1** y **F2** que pueden emitir uno entre cuatro símbolos: $A$, $B$, $C$ o $D$\n",
    "\n",
    "**F1** es completamente aleatoria, es decir: $P(A) = P(B) = P(C) = P(D) = \\frac{1}{4}$\n",
    "\n",
    "Si queremos predecir el próximo valor emitido por **F1** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer?\n",
    "\n",
    "\n",
    "<img src=\"images/information1.svg\" width=\"600\">\n",
    "\n",
    "> La respuesta es 2 para cualquiera de los símbolos\n",
    "\n",
    "**F2** en cambio emite $A$, $B$, $C$ y $D$ con probabilidades $P(A) =\\frac{1}{2}$, $P(B) =\\frac{1}{4}$, $P(C) = \\frac{1}{8}$ y $P(D) =\\frac{1}{8}$, respectivamente\n",
    "\n",
    "Si queremos predecir el próximo valor retornado por **F2** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer? \n",
    "\n",
    "<img src=\"images/information2.svg\" width=\"800\">\n",
    "\n",
    "> La respuesta es 1 para $A$, 2 para $B$ y 3 para $C$ y $D$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cantidad de información (según Shannon)\n",
    "\n",
    "La cantidad de información de un símbolo $x$ es el logaritmo en base dos del recíproco de su probabilidad de aparición\n",
    "\n",
    "$$\n",
    "I(x) = \\log_2 \\left(\\frac{1}{P(x)} \\right) = \\log_2 P(x)^{-1} = - \\log_2 P(x),\n",
    "$$\n",
    "\n",
    "que es equivalente a la mínima cantidad de preguntas si/no que debemos hacer para adivinar su valor\n",
    "\n",
    "La cantidad de información se mide en **bits**\n",
    "\n",
    ">Un **bit** es la cantidad de información que se requiere para escoger entre **dos** alternativas equiprobables\n",
    "\n",
    "La cantidad de información es también llamada **sorpresa**\n",
    "\n",
    "> Mientras más improbable es un símbolo, más nos sorprendemos cuando observamos que ocurre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropía\n",
    "\n",
    "Sea una variable aleatoria $X$ (fuente) con $N$ resultados posibles (símbolos) $\\{x_1, x_2, \\ldots, x_N\\}$\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad $p_i \\in [0, 1]$ y $\\sum_{i=1}^N p_i = 1$ \n",
    "\n",
    "Cada símbolo tiene una cantidad de información  $I(x_i) = -\\log_2 \\left( p_i \\right)$ \n",
    "\n",
    "Definimos la **cantidad de información promedio** de $X$ como\n",
    "$$\n",
    "\\begin{align}\n",
    "H (X) &= \\mathbb{E}_{x\\sim X} \\left [ - \\log P(X=x) \\right ]  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N P(X=x_i) \\log_2 P(X=x_i)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i  \\quad \\text{[bits/símbolo]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "que se conoce como **Entropía de Shannon** \n",
    "\n",
    "### Propiedades\n",
    "- La entropía es siempre positiva $H(X) \\geq 0$. La igualdad se cumple si un $x_i$ tiene $p_i=1$ (caso más predecible)\n",
    "- La entropia está acotada $H(X) \\leq H_0$, donde $H_0= \\log_2(N)$ es la entropia si $p_i = \\frac{1}{N}~ \\forall i$ (caso menos predecible)\n",
    "- La redundancia de $X$ es $1 - H(X)/H_0$\n",
    "\n",
    "> Mientras más predecible es $X$ menor es su entropía y mayor es su redundancia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### El retorno de las dos fuentes\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F1**?\n",
    "\n",
    "> $1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} = 2$ preguntas por símbolo. Su entropía es $2$ [bits]\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F2**?\n",
    "\n",
    "> $1 \\frac{1}{2} + 2 \\frac{1}{4} + 3 \\frac{1}{8} + 3 \\frac{1}{8} = 1.75$ preguntas por símbolo. Su entropía es $1.75$ [bits]\n",
    "\n",
    "Si cada fuente retorna un mensaje de 100 símbolos ¿Cúanta información produjo cada una?\n",
    "\n",
    "> **F1** produce 200 bits mientras que **F2** produce 175 bits\n",
    "\n",
    "Mientras más predecible menos información se necesita\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: Moneda con truco\n",
    "\n",
    "- Sea una variable aleatoria $X$ que modela el resultado de lanzar una moneda\n",
    "- Asumamos que el resultado puede tomar solo dos valores: Cara $o$ o Cruz $x$\n",
    "- La probabilidad de que salga cara es $p_o = p$\n",
    "- La probabilidad de que salga cruz es $p_x = 1- p$\n",
    "- Luego la entropía es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -\\sum_{i=1}^2 p_i \\log_2 p_i \\nonumber \\\\ \n",
    "&= -p_x \\log (p_x) - p_o \\log p_o \\nonumber \\\\\n",
    "&= - p \\log(p) - (1-p) \\log(1-p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reflexione:\n",
    "- ¿En que casos la entropía es mínima? ¿En qué caso es máxima?\n",
    "- ¿Puedes relacionar la entropía con la aleatoridad/sorpresa del resultado de lanzar la moneda?\n",
    "\n",
    "\n",
    "Ojo: $\\lim_{z\\to 0^+} z \\log 1/z = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = np.linspace(0.01, 0.99, num=100)\n",
    "H = -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "fig, ax = plt.subplots(1, figsize=(6, 4), sharex=True)\n",
    "ax.set_xlabel('p')\n",
    "ax.plot(p, -np.log2(p), label='I(o)', lw=3)\n",
    "ax.plot(p, -np.log2(1-p), label='I(x)', lw=3)\n",
    "ax.plot(p, H, label='H(X)', lw=3)\n",
    "ax.set_ylim([0, 3])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "\n",
    "Sea una fuente que escupe un entero x que está entre 0 y 31\n",
    "\n",
    "Considere el resultado de las siguientes preguntas ¿Cúal tiene mayor entropía?\n",
    "- ¿Es x igual a 0?\n",
    "- ¿Es x un número primo?\n",
    "- ¿Es x mayor a 15?\n",
    "\n",
    "¿Cuál es el número mínimo de preguntas con respuesta si/no que se deben hacer para adivinar el valor de x?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: Meteorólogos del siglo XIX\n",
    "\n",
    "- Nos encontramos a finales del siglo XIX\n",
    "- La estación meteorológica de Niebla hace una predicción del tiempo en Valdivia\n",
    "- Esta información se envía a Valdivia a través de telegrafo\n",
    "- Calcule la cantidad de información promedio que envía la estación a Valdivia en cada escenario usando la **entropía de Shannon**\n",
    "\n",
    "**Escenario 1:** Dos posibilidades: Lluvia y nublado, con probabilidad $1/2$ y $1/2$, respectivamente\n",
    "\n",
    "**Escenario 2:** Una posibilidad: Lluvia, con probabilidad $1$\n",
    "\n",
    "**Escenario 3:** Cuatro posibilidades: Lluvia, Nublado, Nubosidad parcial, soleado, con probabilidad $1/2$, $1/4$, $1/8$ y $1/8$, respectivamente\n",
    "\n",
    "1. Las probabilidades de cada mensaje son $2^{-1}$, $2^{-2}$, $2^{-3}$ y $2^{-3}$\n",
    "1. La cantidad de información de cada mensaje es: 1, 2, 3 y 3 bits, respectivamente\n",
    "1. La entropía es $1/2 + 1/2 + 3/8 + 3/8 = 1.75$ bits\n",
    "\n",
    "Para el **escenario 3** códifique las alternativas usando un alfabeto de códigos binarios\n",
    "\n",
    "> ¿Cómo le asignamos un código a cada alternativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Código de ancho fijo\n",
    "\n",
    "- Tenemos cuatro estados, necesitamos 2 bits\n",
    "- Cada estado: 00, 01, 10, 11\n",
    "- En este caso resulta equivalente a asumir equiprobabilidad \n",
    "- La entropía es 2 bits\n",
    "\n",
    "### Código de ancho variable (prefijo)\n",
    "\n",
    "- Se usa 1, 2, 3 y 3 bits para cada estado, según su probabilidad de aparición\n",
    "- La entropía es 1.75 bits\n",
    "- Podemos describir este escenario según\n",
    "    - Primera decisión equiprobable: Lluvia **(0)** vs El resto (1)\n",
    "    - Segunda decisión equiprobable: Nublado **(10)** vs El resto menos lluvia (11)\n",
    "    - Tercera decisión equiprobable: Nubosidad parcial **(110)** vs soleado **(111)**\n",
    "- Podemos escribir esto como un dendograma\n",
    "\n",
    "<center><img src=\"images/dendogram.png\" width=\"600\"></center>\n",
    "\n",
    "- Algoritmo de codificación con forma de árbol en base 2\n",
    "- Los mensajes codificados están en las hojas del árbol\n",
    "- **Código préfijo**: Ningún código puede ser prefijo de otro. \n",
    "- El código prefijo garantiza decodificación sin ambiguedad\n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Decodifique la predicción del tiempo para los próximos tres días: 101100 \n",
    "\n",
    "**Ejemplo de código ambiguo** \n",
    "\n",
    "Si el código de lluvia fuera **1** en lugar de 0, decodifique el siguiente mensaje: 11111\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: Entropía y cantidad de bits del fragmento del famoso texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Código de largo fijo:\n",
    "print(5*len(texto))\n",
    "# Código de largo variable:\n",
    "freq = np.array(list(Counter(texto).values()))\n",
    "p = freq/np.sum(freq)\n",
    "print(int(-np.sum(p*np.log2(p))*len(texto)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Reflexionemos:** \n",
    "- ¿Es la codificación de largo variable *lossless* o *lossy*?\n",
    "- En ciertos casos las palabras son más largas de lo que eran originalmente, ¿Cómo comprimimos entonces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Codificación de Huffman\n",
    "\n",
    "Un algoritmo sencillo de codificación de tipo prefijo:\n",
    "\n",
    "1. Se estima la probabilidad $p_i$ de cada símbolo\n",
    "1. Se ordenan los símbolos en orden descendente según $p_i$\n",
    "1. Juntar los dos con probabilidad menor en un grupo, su probabilidad se suma\n",
    "1. Volver al paso 2 hasta que queden dos grupos\n",
    "1. Asignarle 0 y 1 a las ramas izquierda y derecha del árbol, respectivamente\n",
    "1. El código resultante se lee desde la raiz hasta la hoja\n",
    "\n",
    "<center><a href=\"http://www.skylondaworks.com/sc_huff.htm\"><img src=\"images/huff.gif\" width=\"600\"></a></center>\n",
    "\n",
    "\n",
    "**Debilidad de Huffman:** \n",
    "- Códigos con diccionarios/probabilidades variables\n",
    "- En ese caso combiene usar codificación aritmética o Lempel-Ziv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que en general las señales/datos tienen alta redundancia \n",
    "\n",
    "> Piense por ejemplo en el caso de las imágenes o el lenguaje (contexto)\n",
    "\n",
    "Hemos visto también como comprimir datos\n",
    "\n",
    "En particular \n",
    "1. Transformamos los datos tal de hacerlos \"más independientes\" y opcionalmente los cuantizamos\n",
    "1. Codificamos los datos con una distribución que sea óptima para el canal de transmisión\n",
    "\n",
    "Este último paso es lo que llamamos **codificación de fuente**\n",
    "\n",
    "A continuación revisaremos un importante teorema enunciado por Shannon respecto a la **codificación de un mensaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teorema de codificación de fuente de Shannon (*Source coding theorem*)\n",
    "\n",
    "\n",
    ">Dada una variable aleatoria $X$ con entropía $H(X)$ existe una codificación de largo variable cuyo largo de palabra promedio $\\bar L$ satisface\n",
    "\n",
    ">$$\n",
    "H(X) \\leq \\bar L < H(X) + 1\n",
    "$$\n",
    "\n",
    "Es decir que el límite inferior teórico del largo de palabra es $H(X)$\n",
    "\n",
    "Esta codificación sin pérdida y de largo variable la llamamos **codificación entrópica** \n",
    "\n",
    "Este teorema \n",
    "- nos dice cuanto podemos comprimir una señal sin que hayan pérdidas antes de enviarla por un canal (libre de ruido)\n",
    "- justifica la definición de entropía como medida de la cantidad de información\n",
    "\n",
    "\n",
    "Otra forma de ver el teorema\n",
    "\n",
    "Sea una fuente $X$ que emite $N$ mensajes. \n",
    "\n",
    "> Los N mensajes pueden comprimirse en $N H(X)$ [bits] o más con riesgo de pérdida despreciable ($N\\to\\infty$)\n",
    "\n",
    "Por el contrario\n",
    "\n",
    "> Si comprimimos en menos de $N H(X)$ [bits] la pérdida está garantizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probando el teorema\n",
    "\n",
    "Sea una codificación C para una variable aleatoria $X$ con N posibles símbolos\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad de ocurrencia $p_i \\in [0, 1]$ con $\\sum_i p_i = 1$ y un largo de código $L_i$\n",
    "\n",
    "Luego el largo promedio de los códigos es\n",
    "\n",
    "$$\n",
    "\\bar L = \\sum_{i=1}^N p_i L_i\n",
    "$$\n",
    "\n",
    "¿Qué valores de $L_i$ resultan en el menor $\\bar L$? \n",
    "\n",
    "> El largo óptimo es $L_i^* = -\\log_2 p_i$ y el promedio sería $\\bar L^* = H(X)$\n",
    "\n",
    "Digamos que proponemos otro largo $\\hat L_i = - \\log_2 q_i$, asumiendo que $\\sum_i q_i = 1$\n",
    "\n",
    "Luego el largo promedio sería\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar L &= \\sum_{i=1}^N p_i \\hat L_i  = - \\sum_{i=1}^N p_i \\log_2 q_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 q_i - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 p_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\nonumber \\\\\n",
    "&= H(X) + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\geq H(X) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "> Con esto probamos que no hay mejor largo que $-\\log_2 p_i$\n",
    "\n",
    "Notemos que los $L_i^*$ no tendrían porque ser un número enteros \n",
    "\n",
    "> En general la codificación óptima cumple: $H(X) \\leq \\bar L^* < H(X) + 1$\n",
    "\n",
    "- Se puede estar entre esas cotas con el algoritmo de Huffman\n",
    "- La codificación aritmética en cambio casi siempre llega a la cota inferior\n",
    "- La codificación de Huffman y aritmética son **codificaciones entrópicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergencia de Kullback-Leibler (KL)\n",
    "\n",
    "**Divergencia:** Medida de similitud entre distribuciones estadísticas (No es exactamente una distancia)\n",
    "\n",
    "Sean dos densidades $P=\\{p_1, p_2, \\ldots, p_N\\}$ y $Q=\\{q_1, q_2, \\ldots, q_N\\}$ sobre una misma variable $x$. Su **divergencia de KL** es\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left ( P||Q \\right) &= \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\nonumber \\\\\n",
    "&= -\\sum_{i=1}^N p_i \\log_2 q_i + \\sum_{i=1}^N p_i \\log_2 p_i \\nonumber \\\\\n",
    "&= H_Q(P) -H(P) \\geq 0 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $H(P)$ es la entropía de $P$ \n",
    "- $H_Q(P)$ es la **entropía cruzada** de $P$ con $Q$\n",
    "- La desigualdad final se conoce como [desigualdad de Gibbs](https://en.wikipedia.org/wiki/Gibbs%27_inequality)\n",
    "- La igualdad se cumple sólo si $P=Q$\n",
    "\n",
    "La divergencia de KL también se conoce como la **entropía relativa de P con respecto a Q**\n",
    "\n",
    "\n",
    "> La divergencia de KL nos dice la \"cantidad extra\" de bits necesarios para codificar P usando un código optimizado para Q\n",
    "\n",
    "> La divergencia de KL mide cantidad de información perdida cuando uso Q para aproximar P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información Mutua\n",
    "\n",
    "La información mutua entre dos variables aleatorias $X$ e $Y$ se puede definir de varias maneras\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MI}(X, Y) &= H(X) + H(Y) - H(X, Y)  \\nonumber \\\\\n",
    "&= H(Y) - H(X|Y) = H(X) - H(Y|X)   \\nonumber \\\\\n",
    "&= D_\\text{KL} \\left ( P_{XY} || P_{X} P_{Y} \\right) \\nonumber \\\\\n",
    "&= \\sum_{i} \\sum_{j} P_{XY}(X=x_i, Y=y_j) \\log_2 \\frac{P_{XY}(X=x_i,Y=y_j)}{P_{X}(X=x_i) P_Y(Y=y_j)}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $H(X,Y)$ es la entropía conjunta\n",
    "    - Cantidad de información promedio en bits de $X$ e $Y$\n",
    "- $H(X|Y)$ es la entropía de $X$ condicionada en $Y$\n",
    "    - Cantidad de información promedio en bits de $X$ considerando que conocemos $Y$\n",
    "    - Incerteza de $X$ dado que observamos $Y$\n",
    "    - Recordemos la relación entre densidad conjunta y condicional: $p(x|y)p(y) = p(x,y)$\n",
    "- Se cumple que $H(X)+H(Y) \\geq H(X,Y)$ por lo tanto $\\text{MI}(X,Y) \\geq 0$\n",
    "- Si $X$ e $Y$ son independientes entonces $H(X|Y)=H(X)$ y $H(X,Y) = H(X) + H(Y)$ y $\\text{MI}(X,Y) = 0$\n",
    "\n",
    "> La información mutua mide la información compartida por $X$ e $Y$, es decir que tan dependientes son entre sí\n",
    "\n",
    "> La información mutua nos dice la información promedio en bits que ganamos sobre $Y$ dado que observamos $X$ (y viceverza)\n",
    "\n",
    "> La información mutua nos dice la incerteza promedio de $Y$ que eliminamos al conocer $X$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canal con ruido\n",
    "\n",
    "Hasta ahora hemos asumido que el canal está libre de ruido\n",
    "\n",
    "El objetivo original de Shannon era\n",
    "\n",
    "> \"Comunicación robusta a través de un canal ruidoso\"\n",
    "\n",
    "El ruido disminuye la **capacidad** del canal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Un cisne navegando por  un canal ruidoso\n",
    "\n",
    "Imagemos que queremos transmitir una imagen binaria $X$ por un canal con ruido\n",
    "\n",
    "El canal le cambia el valor a un 10% de los píxeles\n",
    "\n",
    "Llamamos $Y$ a la imagen que sale del canal\n",
    "\n",
    "Este canal de comunicación se conoce como [canal binario simétrico](https://en.wikipedia.org/wiki/Binary_symmetric_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "th_binary, p_noise = 0.5, 0.1\n",
    "img_swan_gray = plt.imread('images/gray-swan.png')[:, :, 0]\n",
    "img_swan = (img_swan_gray > th_binary).astype('uint8')\n",
    "Npix = len(img_swan.ravel())\n",
    "p = np.random.rand(img_swan.shape[0], img_swan.shape[1])\n",
    "img_noisy_swan = img_swan.copy()\n",
    "img_noisy_swan[p <= p_noise] = 1-img_noisy_swan[p <= p_noise]\n",
    "\n",
    "ax[0].imshow(img_swan); ax[0].axis('off')\n",
    "ax[1].imshow(img_noisy_swan); ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la entropía de cada imagen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_binary_image(img):\n",
    "    p = np.count_nonzero(img.ravel())/len(img.ravel())\n",
    "    return -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "\n",
    "HX = entropy_binary_image(img_swan)\n",
    "HY = entropy_binary_image(img_noisy_swan)\n",
    "print(\"Entropía imagen limpia H(X): {0:0.4f} [bits/pixel]\".format(HX))\n",
    "print(\"Entropía imagen sucia H(Y): {0:0.4f} [bits/pixel]\".format(HY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la entropía conjunta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Npix = len(img_swan.ravel())\n",
    "p00 = np.count_nonzero((img_swan == 0) & (img_noisy_swan == 0))/Npix\n",
    "p01 = np.count_nonzero((img_swan == 0) & (img_noisy_swan == 1))/Npix\n",
    "p10 = np.count_nonzero((img_swan == 1) & (img_noisy_swan == 0))/Npix\n",
    "p11 = np.count_nonzero((img_swan == 1) & (img_noisy_swan == 1))/Npix\n",
    "print(np.array([p00, p01, p10, p11]))\n",
    "\n",
    "HXY = -(p00*np.log2(p00) + p01*np.log2(p01) + p10*np.log2(p10) + p11*np.log2(p11))\n",
    "print(\"Entropía conjunta H(X,Y): {0:0.6f} [bits/pixel]\".format(HXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las entropía condicionales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H(X|Y): {0:0.6f} [bits/pixel]\".format(HXY-HY))\n",
    "print(\"H(Y|X): {0:0.6f} [bits/pixel]\".format(HXY-HX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la información mutua?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXY = HX + HY - HXY\n",
    "print(\"Información mutua IM(X,Y): {0:0.6f} [bits/pixel]\".format(MIXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cúal es la entropía del ruido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noise = -p_noise*np.log2(p_noise) - (1-p_noise)*np.log2(1-p_noise)\n",
    "print(\"Entropía del ruido H(N): {0:0.6f} [bits/pixel]\".format(H_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos que $Y = X + N$\n",
    "\n",
    "Entonces\n",
    "$$\n",
    "H(Y|X) = H(X+N|X) = H(N|X) = H(N)\n",
    "$$\n",
    "\n",
    "> En un canal ruidoso la entropía condicional de la salida dada la entrada es equivalente a la entropía del ruido añadido a la entrada\n",
    "\n",
    "<img src=\"images/entropy_mi_diagram.png\">\n",
    "\n",
    "La eficiencia de la transmisión está dada por la proporción de entropía de $Y$ que es compartida por $X$\n",
    "\n",
    "$$\n",
    "E = \\frac{\\text{MI}(X,Y)}{H(Y)} \\in [0, 1]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXY/HY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un 37% de la entropía de la salida depende de la entrada\n",
    "\n",
    "¿Qué ocurre con la información mutua y con la eficiencia de transmisión cuando el canal se vuelve más o menos ruidoso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrección de errores debidos al ruido\n",
    "\n",
    "Queremos ser capaces de recuperar X a partir de Y\n",
    "\n",
    "Cuando el canal tiene ruido necesitamos robustecer el código de X\n",
    "\n",
    "Esto se logra **agregando redundancia** a nuestro código\n",
    "\n",
    "- Enviar el mensaje varias veces: **repetition code**\n",
    "- Agregar al código uno o más **bits de paridad**\n",
    "\n",
    "\n",
    "### Ejemplo: *repetition code*\n",
    "\n",
    "Si queremos enviar 0110011 lo repetimos una cierta cantidad de veces\n",
    "\n",
    "$X$ = 000 111 111 000 000 111 111 \n",
    "\n",
    "$N$ = 001 000 000 000 000 110 000\n",
    "\n",
    "$Y$ = 001 111 111 000 001 001 111\n",
    "\n",
    "Si aplicamos decodificación por mayoría: 0 1 1 0 0 **0** 1\n",
    "\n",
    "Reducimos la probabilidad de error por un factor 3\n",
    "\n",
    "La tasa es $R = k/n = 3$ donde $k$ son los bits de información y $n$ los bits transmitidos\n",
    "\n",
    "### Ejemplo: paridad\n",
    "\n",
    "Sea una tira binaria de 16 bits $s=[0,1,0,1,0,0,0,0,1,1,1,0, 0,1,1,0]$ Se ordena como una matriz de 4x4\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c c c c|c}\n",
    "  0 & 1 & 0 & 1 & 0\\\\\n",
    "  0 & 0 & 0 & 0 & 0\\\\\n",
    "  1 & 1 & 1 & 0 & 1\\\\\n",
    "  0 & 1 & 1 & 0 & 0 \\\\ \\hline\n",
    "  1 & 1 & 0 & 1 & \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Si el número de 1's de una fila o columna es par se agrega un 0, de lo contrario se agrega un 1\n",
    "\n",
    "Finalmente se crea una nueva tira de largo 24\n",
    "\n",
    "$s_p=[0,1,0,1,\\textbf{0}, 0,0,0,0,\\textbf{0}, 1,1,1,0, \\textbf{1}, 0,1,1,0, \\textbf{0}, \\textbf{1}, \\textbf{1}, \\textbf{0}, \\textbf{1}]$ \n",
    "\n",
    "- Al recibir este código se comprueba que las paridades estén bien\n",
    "- Si no lo están podriamos pedir nuevamente la tira binaria\n",
    "- Aumentamos el mensaje de 16 a 24 [bits], la tasa es $R=16/24=0.666$\n",
    "\n",
    "**Ejercicio:** Si tengo un string de NxN de largo y quiero proteger con bits de paridad ¿Cúantos bits agrego?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teorema de codificación de canal de Shannon (*Channel coding theorem*)\n",
    "\n",
    "Se define la capacidad de un canal con entrada $X$ y salida $Y$ como\n",
    "\n",
    "$$\n",
    "C = \\max_{P(X)} \\text{MI}(X,Y) ~\\text{[bits/símbolo]} \n",
    "$$\n",
    "\n",
    "La distribución $P^*(X)$ que maximiza la capacidad del canal es la distribución de entrada óptima para ese canal\n",
    "\n",
    "Notemos que si el canal no tiene ruido entonces $Y=X$ y la capacidad está dada por la entropía de $X$\n",
    "\n",
    "> El ruido disminuye la capacidad de un canal\n",
    "\n",
    "Al respecto Shannon enunció el siguiente teorema\n",
    "\n",
    "> Sea un canal con capacidad $C$ y una fuente $X$ que transmite a una taza de $R$\n",
    "\n",
    ">Si $R \\leq C$ entonces existe un un largo de codificación para $X$ que puede transmitirse con error arbitrariamente pequeño\n",
    "\n",
    "> Para una una probabilidad de error de bit aceptable $p_e$, se puede alcanzar una taza de transmisión\n",
    "$$\n",
    "R(p_e) = \\frac{1}{1 + p_e \\log_2(p_e) + (1-p_e) \\log_2(1-p_e)}\n",
    "$$\n",
    "\n",
    "> Para un cierto $p_e$ no es posible alcanzar una taza mayor a $R(p_e)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Canal binario simétrico\n",
    "\n",
    "¿Cuál es la distribución de entrada que maximiza la información mutua del canal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_flip, tol = 0.1, 1e-2\n",
    "np.random.seed(0)\n",
    "seed_image = np.random.rand(img_swan_gray.shape[0], img_swan_gray.shape[1])\n",
    "flip_mask = np.random.rand(img_swan_gray.shape[0], img_swan_gray.shape[1]) <= p_flip\n",
    "\n",
    "binarization_threshold = np.linspace(0.01, 0.999, num=100)\n",
    "MIXY = []\n",
    "for th in binarization_threshold:\n",
    "    img_X = (img_swan_gray > th).astype('uint8')\n",
    "    img_Y = img_X.copy()\n",
    "    img_Y[flip_mask] = 1-img_Y[flip_mask] \n",
    "    p00 = np.count_nonzero((img_X == 0) & (img_Y == 0))/Npix\n",
    "    p01 = np.count_nonzero((img_X == 0) & (img_Y == 1))/Npix\n",
    "    p10 = np.count_nonzero((img_X == 1) & (img_Y == 0))/Npix\n",
    "    p11 = np.count_nonzero((img_X == 1) & (img_Y == 1))/Npix\n",
    "    HX = -(p00 + p01)*np.log2(p00 + p01+tol) -(p10 + p11)*np.log2(p10 + p11+tol)\n",
    "    HY = -(p00 + p10)*np.log2(p00 + p10+tol) -(p01 + p11)*np.log2(p01 + p11+tol)\n",
    "    HXY= -(p00*np.log2(p00+tol) + p01*np.log2(p01+tol) + p10*np.log2(p10+tol) + p11*np.log2(p11+tol))\n",
    "    MIXY.append(HX + HY - HXY)\n",
    "    \n",
    "fig, ax = plt.subplots(2, 2, figsize=(9, 7), tight_layout=True)\n",
    "ax[1, 0].axis('off'); ax[0, 1].axis('off'); ax[1, 1].set_xticks([0, 1]); \n",
    "ax[1,1].set_xlabel('X'); ax[1, 1].set_ylabel('P(X)');\n",
    "ax[0, 0].set_ylabel('Información mutua')\n",
    "ax[0, 0].set_xlabel('Umbral de binarización');\n",
    "best_th = binarization_threshold[np.argmax(MIXY)]\n",
    "ax[0, 1].text(0.0,0.5, \"Mejor umbral: {0:0.4f}\\nMejor MI(X,Y): {1:0.4f}\".format(best_th, np.amax(MIXY)))\n",
    "ax[0, 0].plot(binarization_threshold, MIXY); \n",
    "ax[1, 0].imshow((img_swan_gray > best_th).astype('uint8')); \n",
    "ax[1, 1].hist((img_swan_gray > best_th).astype('uint8').ravel(), \n",
    "              align='mid', range=[-0.5, 1.5],\n",
    "              bins=2, density=True, histtype='bar', ec='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que la entrada $X$ tiene probabilidad $p$ de ser 0 y $(1-p)$ de ser 1 (imagen binaria) \n",
    "\n",
    "Es decir las *probabilidades a priori* son $P(X=0)=p$, $P(X=1)=(1-p)$ \n",
    "\n",
    "Dijimos que el canal cambia un 10\\% de los pixeles de $X$\n",
    "\n",
    "Entonces las verosimilitudes son $P(Y=1|X=1) = 0.9$, $P(Y=1|X=0) = 0.1$, $P(Y=0|X=1) = 0.1$, $P(Y=0|X=0) = 0.9$\n",
    "\n",
    "Las probabilidades marginales de la salida son:\n",
    "- $P(Y=1) = \\sum_x P(Y=1|X=x)P(X=x)  = 0.9(1-p) + 0.1p = 0.9 - 0.8p $\n",
    "- $P(Y=0) = \\sum_x P(Y=0|X=x)P(X=x)  = 0.1(1-p) + 0.9p = 0.1 + 0.8p= 1 - P(Y=1) $ \n",
    "\n",
    "y su entropía es $H(Y) = - (0.9 - 0.8p) \\log_2(0.9 - 0.8p) - (0.1 + 0.8p) \\log_2(0.1 + 0.8p)$\n",
    "\n",
    "\n",
    "La entropía condicional es \n",
    "$$\n",
    "H(Y|X) = \\sum_x H(Y|X=x)P(X=x) = - 0.9 \\log_2(0.9) - 0.1 \\log_2 (0.1)\n",
    "$$\n",
    "que no depende de $p$\n",
    "\n",
    "El máximo de la información mutua:\n",
    "$$\n",
    "\\frac{d}{dp} \\text{MI}(X,Y) = \\frac{d}{dp} H(Y) - \\frac{d}{dp} H(Y|X) = \\frac{d}{dp} H(Y) = 0 \\rightarrow p^*=\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Notemos que no importa que porcentaje de corrupción tenga el canal.\n",
    "\n",
    "Finalmente la capacidad del canal es\n",
    "\n",
    "$$\n",
    "C = \\text{MI}_p^* (X,Y) = 1 + 0.9 \\log_2(0.9) + 0.1 \\log_2 (0.1) = 0.531 \\text{[bits]}\n",
    "$$\n",
    "\n",
    "> Si usamos P^* podemos transmitir sin perdidas a una taza $R=k/n=0.531$, es decir que por $k$ bits de información tenemos que transmitir aproximadamente $2k$ bits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Teorema de Shannon-Hartley\n",
    "\n",
    "- Sea un canal con ancho de banda B [Hz] (rango de frecuencias que un canal puede transmitir) y potencia de señal S [W] y potencia del ruido N [W] (aditivo blanco gaussiano), entonces su capacidad es\n",
    "\n",
    "$$\n",
    "C = B \\log_2 \\left(1 + \\frac{S}{N} \\right) \\text{[bits/s]}\n",
    "$$\n",
    "\n",
    "- Si la velocidad de transmisión de un canal es R [bits/s] y R < C entonces la probabilidad de errores de comunicación tiende a cero.\n",
    "- Las limitaciones de un sistema de comunicación son **Ancho de banda** y el ruido **Ruido**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "http://home.iitk.ac.in/~adrish/Shannon/information_theory.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

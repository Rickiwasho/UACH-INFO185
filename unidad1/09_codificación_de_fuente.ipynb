{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoría de la Información - parte 1\n",
    "\n",
    "## Contenidos\n",
    "\n",
    "- Cantidad de información y entropía\n",
    "- Codificación de fuente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Codificación de fuente\n",
    "\n",
    "> Es el proceso que asigna un código a cada símbolo del diccionario\n",
    "\n",
    "\n",
    "**Ejemplo:** Queremos codificar las letras del alfabeto (27) usando código binario\n",
    "\n",
    "Si usamos un código de largo fijo necesitariamos al menos 5 bits: $2^5 = 32 > 27$\n",
    "\n",
    "> Pero algunas letras se ocupan más que otras ¿Podemos aprovechar esto para comprimir un mensaje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lentejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda. El resto della concluían sayo de velarte, calzas de velludo para las fiestas con sus pantuflos de lo mismo, los días de entre semana se honraba con su vellori de lo más fino. Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años, era de complexión recia, seco de carnes, enjuto de rostro; gran madrugador y amigo de la caza. Quieren decir que tenía el sobrenombre de Quijada o Quesada (que en esto hay alguna diferencia en los autores que deste caso escriben), aunque por conjeturas verosímiles se deja entender que se llama Quijana; pero esto importa poco a nuestro cuento; basta que en la narración dél no se salga un punto de la verdad.\n",
      "\n",
      "Es, pues, de saber, que este sobredicho hidalgo, los ratos que estaba ocioso (que eran los más del año) se daba a leer libros de caballerías con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza, y aun la administración de su hacienda; y llegó a tanto su curiosidad y desatino en esto, que vendió muchas hanegas de tierra de sembradura, para comprar libros de caballerías en que leer; y así llevó a su casa todos cuantos pudo haber dellos; y de todos ningunos le parecían tan bien como los que compuso el famoso Feliciano de Silva: porque la claridad de su prosa, y aquellas intrincadas razones suyas, le parecían de perlas; y más cuando llegaba a leer aquellos requiebros y cartas de desafío, donde en muchas partes hallaba escrito: la razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura, y también cuando leía: los altos cielos que de vuestra divinidad divinamente con las estrellas se fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza. Con estas y semejantes razones perdía el pobre caballero el juicio, y desvelábase por entenderlas, y desentrañarles el sentido, que no se lo sacara, ni las entendiera el mismo Aristóteles, si resucitara para sólo ello. No estaba muy bien con las heridas que don Belianis daba y recibía, porque se imaginaba que por grandes maestros que le hubiesen curado, no dejaría de tener el rostro y todo el cuerpo lleno de cicatrices y señales; pero con todo alababa en su autor aquel acabar su libro con la promesa de aquella inacabable aventura, y muchas veces le vino deseo de tomar la pluma, y darle fin al pie de la letra como allí se promete; y sin duda alguna lo hiciera, y aun saliera con ello, si otros mayores y continuos pensamientos no se lo estorbaran.\n",
      "\n",
      "Largo del texto: 2366, Cantidad de bits: 11830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 317),\n",
       " ('e', 315),\n",
       " ('o', 201),\n",
       " ('s', 190),\n",
       " ('n', 161),\n",
       " ('l', 160),\n",
       " ('r', 151),\n",
       " ('d', 123),\n",
       " ('u', 113),\n",
       " ('i', 110),\n",
       " ('c', 100),\n",
       " ('t', 89),\n",
       " ('m', 61),\n",
       " ('b', 46),\n",
       " ('q', 41),\n",
       " ('p', 41),\n",
       " ('y', 34),\n",
       " ('g', 26),\n",
       " ('h', 23),\n",
       " ('v', 23),\n",
       " ('f', 15),\n",
       " ('z', 14),\n",
       " ('j', 11),\n",
       " ('x', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "with open('../quijote.txt', 'r') as file:\n",
    "    texto = file.read()\n",
    "print(texto)\n",
    "texto = texto.translate({ord(k): None for k in string.digits})\n",
    "for symbol in string.punctuation:\n",
    "    texto = texto.replace(symbol, \"\")\n",
    "texto = texto.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "texto = texto.lower().encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(\"\\nLargo del texto: {0}, Cantidad de bits: {1}\".format(len(texto), len(texto)*5))\n",
    "Counter(texto).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Intuición:** Podríamos reducir la cantidad de bits si usamos códigos más cortos para las letras más frecuentes\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teoría de la información\n",
    "\n",
    "Estudio matemático sobre la cuantificación y transmisión de la información \n",
    "\n",
    "Fue propuesto por **Claude Shannon** en 1948: *A Mathematical Theory of Communication*\n",
    "\n",
    "Proporciona medidas para describir la información de un proceso: **Entropía** e **Información Mutua**\n",
    "\n",
    "Tiene aplicaciones en telecomunicaciones, computación y biología (genética)\n",
    "\n",
    "Fuerte influencia en la teoría de codificación y compresión\n",
    "\n",
    "### Las dos fuentes\n",
    "\n",
    "Sean dos fuentes **F1** y **F2** que pueden emitir uno entre cuatro símbolos: $A$, $B$, $C$ o $D$\n",
    "\n",
    "**F1** es completamente aleatoria, es decir: $P(A) = P(B) = P(C) = P(D) = \\frac{1}{4}$\n",
    "\n",
    "Si queremos predecir el próximo valor emitido por **F1** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer?\n",
    "\n",
    "\n",
    "<img src=\"../images/information1.svg\" width=\"600\">\n",
    "\n",
    "> La respuesta es 2 para cualquiera de los símbolos\n",
    "\n",
    "**F2** en cambio emite $A$, $B$, $C$ y $D$ con probabilidades $P(A) =\\frac{1}{2}$, $P(B) =\\frac{1}{4}$, $P(C) = \\frac{1}{8}$ y $P(D) =\\frac{1}{8}$, respectivamente\n",
    "\n",
    "Si queremos predecir el próximo valor retornado por **F2** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer? \n",
    "\n",
    "<img src=\"../images/information2.svg\" width=\"800\">\n",
    "\n",
    "> La respuesta es 1 para $A$, 2 para $B$ y 3 para $C$ y $D$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cantidad de información (según Shannon)\n",
    "\n",
    "La cantidad de información de un símbolo $x$ es el logaritmo en base dos del recíproco de su probabilidad de aparición\n",
    "\n",
    "$$\n",
    "I(x) = \\log_2 \\left(\\frac{1}{P(x)} \\right) = \\log_2 P(x)^{-1} = - \\log_2 P(x),\n",
    "$$\n",
    "\n",
    "que es equivalente a la mínima cantidad de preguntas si/no que debemos hacer para adivinar su valor\n",
    "\n",
    "La cantidad de información se mide en **bits**\n",
    "\n",
    ">Un **bit** es la cantidad de información que se requiere para escoger entre **dos** alternativas equiprobables\n",
    "\n",
    "La cantidad de información es también llamada **sorpresa**\n",
    "\n",
    "> Mientras más improbable es un símbolo, más nos sorprendemos cuando observamos que ocurre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropía\n",
    "\n",
    "Sea una variable aleatoria $X$ (fuente) con $N$ resultados posibles (símbolos) $\\{x_1, x_2, \\ldots, x_N\\}$\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad $p_i \\in [0, 1]$ y $\\sum_{i=1}^N p_i = 1$ \n",
    "\n",
    "Cada símbolo tiene una cantidad de información  $I(x_i) = -\\log_2 \\left( p_i \\right)$ \n",
    "\n",
    "Definimos la **cantidad de información promedio** de $X$ como\n",
    "$$\n",
    "\\begin{align}\n",
    "H (X) &= \\mathbb{E}_{x\\sim X} \\left [ - \\log P(X=x) \\right ]  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N P(X=x_i) \\log_2 P(X=x_i)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i  \\quad \\text{[bits/símbolo]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "que se conoce como **Entropía de Shannon** \n",
    "\n",
    "### Propiedades\n",
    "- La entropía es siempre positiva $H(X) \\geq 0$. La igualdad se cumple si un $x_i$ tiene $p_i=1$ (caso más predecible)\n",
    "- La entropia está acotada $H(X) \\leq H_0$, donde $H_0= \\log_2(N)$ es la entropia si $p_i = \\frac{1}{N}~ \\forall i$ (caso menos predecible)\n",
    "- La redundancia de $X$ es $1 - H(X)/H_0$\n",
    "\n",
    "> Mientras más predecible es $X$ menor es su entropía y mayor es su redundancia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### El retorno de las dos fuentes\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F1**?\n",
    "\n",
    "> $1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} = 2$ preguntas por símbolo. Su entropía es $2$ [bits]\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F2**?\n",
    "\n",
    "> $1 \\frac{1}{2} + 2 \\frac{1}{4} + 3 \\frac{1}{8} + 3 \\frac{1}{8} = 1.75$ preguntas por símbolo. Su entropía es $1.75$ [bits]\n",
    "\n",
    "Si cada fuente retorna un mensaje de 100 símbolos ¿Cúanta información produjo cada una?\n",
    "\n",
    "> **F1** produce 200 bits mientras que **F2** produce 175 bits\n",
    "\n",
    "Mientras más predecible menos información se necesita\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo 1: Moneda con truco\n",
    "\n",
    "- Sea una variable aleatoria $X$ que modela el resultado de lanzar una moneda\n",
    "- Asumamos que el resultado puede tomar solo dos valores: Cara $o$ o Cruz $x$\n",
    "- La probabilidad de que salga cara es $p_o = p$\n",
    "- La probabilidad de que salga cruz es $p_x = 1- p$\n",
    "- Luego la entropía es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -\\sum_{i=1}^2 p_i \\log_2 p_i \\nonumber \\\\ \n",
    "&= -p_x \\log (p_x) - p_o \\log p_o \\nonumber \\\\\n",
    "&= - p \\log(p) - (1-p) \\log(1-p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reflexione:\n",
    "- ¿En que casos la entropía es mínima? ¿En qué caso es máxima?\n",
    "- ¿Puedes relacionar la entropía con la aleatoridad/sorpresa del resultado de lanzar la moneda?\n",
    "\n",
    "\n",
    "Ojo: $\\lim_{z\\to 0^+} z \\log 1/z = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "p = np.linspace(0.01, 0.99, num=100)\n",
    "H = -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "fig, ax = plt.subplots(1, figsize=(6, 4), sharex=True)\n",
    "ax.set_xlabel('p')\n",
    "ax.plot(p, -np.log2(p), label='I(o)', lw=3)\n",
    "ax.plot(p, -np.log2(1-p), label='I(x)', lw=3)\n",
    "ax.plot(p, H, label='H(X)', lw=3)\n",
    "ax.set_ylim([0, 3])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "\n",
    "Sea una fuente que escupe un entero x que está entre 0 y 31\n",
    "\n",
    "Considere el resultado de las siguientes preguntas ¿Cúal tiene mayor entropía?\n",
    "- ¿Es x igual a 0?\n",
    "- ¿Es x un número primo?\n",
    "- ¿Es x mayor a 15?\n",
    "\n",
    "¿Cuál es el número mínimo de preguntas con respuesta si/no que se deben hacer para adivinar el valor de x?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo 2: Meteorólogos del siglo XIX\n",
    "\n",
    "- Nos encontramos a finales del siglo XIX\n",
    "- La estación meteorológica de Niebla hace una predicción del tiempo en Valdivia\n",
    "- Esta información se envía a Valdivia a través de telegrafo\n",
    "- Calcule la cantidad de información promedio que envía la estación a Valdivia en cada escenario usando la **entropía de Shannon**\n",
    "\n",
    "**Escenario 1:** Dos posibilidades: Lluvia y nublado, con probabilidad $1/2$ y $1/2$, respectivamente\n",
    "\n",
    "**Escenario 2:** Una posibilidad: Lluvia, con probabilidad $1$\n",
    "\n",
    "**Escenario 3:** Cuatro posibilidades: Lluvia, Nublado, Nubosidad parcial, soleado, con probabilidad $1/2$, $1/4$, $1/8$ y $1/8$, respectivamente\n",
    "\n",
    "1. Las probabilidades de cada mensaje son $2^{-1}$, $2^{-2}$, $2^{-3}$ y $2^{-3}$\n",
    "1. La cantidad de información de cada mensaje es: 1, 2, 3 y 3 bits, respectivamente\n",
    "1. La entropía es $1/2 + 1/2 + 3/8 + 3/8 = 1.75$ bits\n",
    "\n",
    "Para el **escenario 3** códifique las alternativas usando un alfabeto de códigos binarios\n",
    "\n",
    "> ¿Cómo le asignamos un código a cada alternativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Código de ancho fijo\n",
    "\n",
    "- Tenemos cuatro estados, necesitamos 2 bits\n",
    "- Cada estado: 00, 01, 10, 11\n",
    "- En este caso resulta equivalente a asumir equiprobabilidad \n",
    "- La entropía es 2 bits\n",
    "\n",
    "### Código de ancho variable (prefijo)\n",
    "\n",
    "- Se usa 1, 2, 3 y 3 bits para cada estado, según su probabilidad de aparición\n",
    "- La entropía es 1.75 bits\n",
    "- Podemos describir este escenario según\n",
    "    - Primera decisión equiprobable: Lluvia **(0)** vs El resto (1)\n",
    "    - Segunda decisión equiprobable: Nublado **(10)** vs El resto menos lluvia (11)\n",
    "    - Tercera decisión equiprobable: Nubosidad parcial **(110)** vs soleado **(111)**\n",
    "- Podemos escribir esto como un dendograma\n",
    "\n",
    "<center><img src=\"../images/dendogram.png\" width=\"600\"></center>\n",
    "\n",
    "- Algoritmo de codificación con forma de árbol en base 2\n",
    "- Los mensajes codificados están en las hojas del árbol\n",
    "- **Código préfijo**: Ningún código puede ser prefijo de otro. \n",
    "- El código prefijo garantiza decodificación sin ambiguedad\n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Decodifique la predicción del tiempo para los próximos tres días: 101100 \n",
    "\n",
    "**Ejemplo de código ambiguo** \n",
    "\n",
    "Si el código de lluvia fuera **1** en lugar de 0, decodifique el siguiente mensaje: 11111\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo 3: Entropía y cantidad de bits del fragmento del famoso texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11830"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Código de largo fijo:\n",
    "display(5*len(texto))\n",
    "# Código de largo variable:\n",
    "freq = np.array(list(Counter(texto).values()))\n",
    "p = freq/np.sum(freq)\n",
    "display(int(-np.sum(p*np.log2(p))*len(texto)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Reflexionemos:** \n",
    "- ¿Es la codificación de largo variable *lossless* o *lossy*?\n",
    "- En ciertos casos las palabras son más largas de lo que eran originalmente, ¿Cómo comprimimos entonces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Codificación de Huffman\n",
    "\n",
    "Un algoritmo sencillo de codificación de tipo prefijo:\n",
    "\n",
    "1. Se estima la probabilidad $p_i$ de cada símbolo\n",
    "1. Se ordenan los símbolos en orden descendente según $p_i$\n",
    "1. Juntar los dos con probabilidad menor en un grupo, su probabilidad se suma\n",
    "1. Volver al paso 2 hasta que queden dos grupos\n",
    "1. Asignarle 0 y 1 a las ramas izquierda y derecha del árbol, respectivamente\n",
    "1. El código resultante se lee desde la raiz hasta la hoja\n",
    "\n",
    "<center><a href=\"http://www.skylondaworks.com/sc_huff.htm\"><img src=\"../images/huff.gif\" width=\"600\"></a></center>\n",
    "\n",
    "\n",
    "**Debilidad de Huffman:** \n",
    "- Códigos con diccionarios/probabilidades variables\n",
    "- En ese caso combiene usar codificación aritmética o Lempel-Ziv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4: Codificación de Huffman del famoso texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "https://medium.com/iecse-hashtag/huffman-coding-compression-basics-in-python-6653cdb4c476\n",
    "    \n",
    "https://bhrigu.me/blog/2017/01/17/huffman-coding-python-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Códificación de Huffman en JPEG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import fftpack\n",
    "im = cv2.imread(\"../images/lobo.jpg\")\n",
    "im_bw = cv2.cvtColor(im, cv2.COLOR_BGR2YCrCb)[:, :, 0]\n",
    "DCT2 = lambda g, norm='ortho': fftpack.dct( fftpack.dct(g, axis=0, norm=norm), axis=1, norm=norm)\n",
    "\n",
    "block_dct = DCT2(im_bw[:8,:8])\n",
    "\n",
    "Q = np.array([[16, 11, 10, 16, 24, 40, 51, 61],\n",
    "              [12, 12, 14, 19, 26, 58, 60, 55],\n",
    "              [14, 13, 16, 24, 40, 57, 69, 56],\n",
    "              [14, 17, 22, 29, 51, 87, 80, 62],\n",
    "              [18, 22, 37, 56, 68, 109, 103, 77],\n",
    "              [24, 35, 55, 64, 81, 104, 113, 92],\n",
    "              [49, 64, 78, 87, 103, 121, 120, 101],\n",
    "              [72, 92, 95, 98, 112, 100, 103, 99]])\n",
    "\n",
    "block_dct_q = np.round(block_dct/Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que en general las señales/datos tienen alta redundancia \n",
    "\n",
    "> Piense por ejemplo en el caso de las imágenes o el lenguaje (contexto)\n",
    "\n",
    "Hemos visto también como comprimir datos\n",
    "\n",
    "En particular \n",
    "1. Transformamos los datos tal de hacerlos \"más independientes\"\n",
    "1. Opcionalmente los cuantizamos para eliminar información menos importante\n",
    "1. Codificamos los datos con una distribución que sea óptima para el canal de transmisión\n",
    "\n",
    "Este último paso es lo que llamamos **codificación de fuente**\n",
    "\n",
    "A continuación revisaremos un importante teorema enunciado por Shannon respecto a la **codificación de un mensaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teorema de codificación de fuente de Shannon (*Source coding theorem*)\n",
    "\n",
    "\n",
    ">Dada una variable aleatoria $X$ con entropía $H(X)$ existe una codificación de largo variable cuyo largo de palabra promedio $\\bar L$ satisface\n",
    "\n",
    ">$$\n",
    "H(X) \\leq \\bar L < H(X) + 1\n",
    "$$\n",
    "\n",
    "Es decir que el límite inferior teórico del largo de palabra es $H(X)$\n",
    "\n",
    "Esta codificación sin pérdida y de largo variable la llamamos **codificación entrópica** \n",
    "\n",
    "Este teorema \n",
    "- nos dice cuanto podemos comprimir una señal sin que hayan pérdidas antes de enviarla por un canal (libre de ruido)\n",
    "- justifica la definición de entropía como medida de la cantidad de información\n",
    "\n",
    "\n",
    "Otra forma de ver el teorema\n",
    "\n",
    "Sea una fuente $X$ que emite $N$ mensajes. \n",
    "\n",
    "> Los N mensajes pueden comprimirse en $N H(X)$ [bits] o más con riesgo de pérdida despreciable ($N\\to\\infty$)\n",
    "\n",
    "Por el contrario\n",
    "\n",
    "> Si comprimimos en menos de $N H(X)$ [bits] la pérdida está garantizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probando el teorema\n",
    "\n",
    "Sea una codificación C para una variable aleatoria $X$ con N posibles símbolos\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad de ocurrencia $p_i \\in [0, 1]$ con $\\sum_i p_i = 1$ y un largo de código $L_i$\n",
    "\n",
    "Luego el largo promedio de los códigos es\n",
    "\n",
    "$$\n",
    "\\bar L = \\sum_{i=1}^N p_i L_i\n",
    "$$\n",
    "\n",
    "¿Qué valores de $L_i$ resultan en el menor $\\bar L$? \n",
    "\n",
    "> El largo óptimo es $L_i^* = -\\log_2 p_i$ y el promedio sería $\\bar L^* = H(X)$\n",
    "\n",
    "Digamos que proponemos otro largo $\\hat L_i = - \\log_2 q_i$, asumiendo que $\\sum_i q_i = 1$\n",
    "\n",
    "Luego el largo promedio sería\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar L &= \\sum_{i=1}^N p_i \\hat L_i  = - \\sum_{i=1}^N p_i \\log_2 q_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 q_i - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 p_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\nonumber \\\\\n",
    "&= H(X) + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\geq H(X) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "> Con esto probamos que no hay mejor largo que $-\\log_2 p_i$\n",
    "\n",
    "Notemos que los $L_i^*$ no tendrían porque ser un número enteros \n",
    "\n",
    "> En general la codificación óptima cumple: $H(X) \\leq \\bar L^* < H(X) + 1$\n",
    "\n",
    "- Se puede estar entre esas cotas con el algoritmo de Huffman\n",
    "- La codificación aritmética en cambio casi siempre llega a la cota inferior\n",
    "- La codificación de Huffman y aritmética son **codificaciones entrópicas**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
